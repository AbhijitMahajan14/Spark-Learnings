{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c995406a-90cc-48e5-bf4e-941e3e5d5260",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-------------+\n| name|age|         city|\n+-----+---+-------------+\n|Alice| 30|     New York|\n|  Bob| 25|San Francisco|\n+-----+---+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"SchemaExample\").getOrCreate()\n",
    "\n",
    "# Define the schema\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),   # Name column, nullable\n",
    "    StructField(\"age\", IntegerType(), False),  # Age column, not nullable\n",
    "    StructField(\"city\", StringType(), True)    # City column, nullable\n",
    "])\n",
    "\n",
    "# Create some sample data\n",
    "data = [(\"Alice\", 30, \"New York\"), (\"Bob\", 25, \"San Francisco\")]\n",
    "\n",
    "# Create DataFrame with schema\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "# Show DataFrame\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5986df3-3995-415d-815c-23270d77ff3c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-------------------+---------------+\n|name |age|address            |skills         |\n+-----+---+-------------------+---------------+\n|Alice|30 |{New York, NY}     |[Python, Spark]|\n|Bob  |25 |{San Francisco, CA}|[Java, Hadoop] |\n+-----+---+-------------------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import ArrayType\n",
    "\n",
    "# Define a nested schema for address\n",
    "address_schema = StructType([\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"state\", StringType(), True),\n",
    "])\n",
    "\n",
    "# Define the main schema with an embedded struct and an array\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), False),\n",
    "    StructField(\"address\", address_schema, True),  # Nested struct\n",
    "    StructField(\"skills\", ArrayType(StringType()), True)  # Array of strings\n",
    "])\n",
    "\n",
    "data = [\n",
    "    (\"Alice\", 30, (\"New York\", \"NY\"), [\"Python\", \"Spark\"]),\n",
    "    (\"Bob\", 25, (\"San Francisco\", \"CA\"), [\"Java\", \"Hadoop\"])\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4872869-32c8-4bb1-bc01-18da39539de1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+-----------+\n|dept|sum(salary)|avg(salary)|\n+----+-----------+-----------+\n|  HR|      11000|     5500.0|\n|  IT|       7000|     7000.0|\n+----+-----------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sum, avg\n",
    "\n",
    "data = [(\"Alice\", \"HR\", 5000), (\"Bob\", \"HR\", 6000), (\"Charlie\", \"IT\", 7000)]\n",
    "df = spark.createDataFrame(data, [\"name\", \"dept\", \"salary\"])\n",
    "\n",
    "# Group by department and calculate sum and average salary\n",
    "aggregated_df = df.groupBy(\"dept\").agg(\n",
    "    sum(\"salary\"),\n",
    "    avg(\"salary\")\n",
    ")\n",
    "aggregated_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd9e5b5c-5423-42e4-b5e7-78fb07c7bdc9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----+\n| id| name|city|\n+---+-----+----+\n|  1|Alice|  NY|\n+---+-----+----+\n\n+---+-----+----+\n| id| name|city|\n+---+-----+----+\n|  1|Alice|  NY|\n|  2|  Bob|null|\n+---+-----+----+\n\n+---+-----+----+\n| id| name|city|\n+---+-----+----+\n|  1|Alice|  NY|\n|  3| null|  LA|\n+---+-----+----+\n\n+---+-----+----+\n| id| name|city|\n+---+-----+----+\n|  1|Alice|  NY|\n|  2|  Bob|null|\n|  3| null|  LA|\n+---+-----+----+\n\n+---+-----+---+----+\n| id| name| id|city|\n+---+-----+---+----+\n|  1|Alice|  1|  NY|\n|  1|Alice|  3|  LA|\n|  2|  Bob|  1|  NY|\n|  2|  Bob|  3|  LA|\n+---+-----+---+----+\n\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame([(1, \"Alice\"), (2, \"Bob\")], [\"id\", \"name\"])\n",
    "df2 = spark.createDataFrame([(1, \"NY\"), (3, \"LA\")], [\"id\", \"city\"])\n",
    "\n",
    "\n",
    "df1.join(df2,on=[\"id\"],how=\"inner\").show()\n",
    "df1.join(df2,on=[\"id\"],how=\"left\").show()\n",
    "df1.join(df2,on=[\"id\"],how=\"right\").show()\n",
    "df1.join(df2,on=[\"id\"],how=\"full\").show()\n",
    "df1.crossJoin(df2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe9a4025-1442-4a94-8db7-20baf8bc0c9b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Define Schema",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
